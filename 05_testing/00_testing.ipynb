{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://files.realpython.com/media/YXhT6fA.d277d5317026.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# with open(\"mynote.ipynb\", \"w\") as f:\n",
    "#     f.write(requests.get(\"https://raw.githubusercontent.com/polyrand/teach/master/05_testing/testing.ipynb\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "import ipytest\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "\n",
    "__file__ = \"testing.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [1, 2, 3] == [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "La variable num debe ser un entero pero es: 1.25\nassert False\n +  where False = isinstance(1.25, int)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Projects/teach/05_testing/testing.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"La variable num debe ser un entero pero es: {num}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: La variable num debe ser un entero pero es: 1.25\nassert False\n +  where False = isinstance(1.25, int)"
     ]
    }
   ],
   "source": [
    "num = 1.25\n",
    "assert isinstance(num, int), f\"La variable num debe ser un entero pero es: {num}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example():\n",
    "    assert [1, 2, 3] == [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_case(x):\n",
    "    \n",
    "    return x.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Un texto'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_case(\"un texto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.8.6, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /Users/r/Projects/teach/05_testing\n",
      "collected 1 item\n",
      "\n",
      "testing.py .                                                             [100%]\n",
      "\n",
      "============================== 1 passed in 0.21s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_capital_case():\n",
    "    assert capital_case(\"semaphore\") == \"Semaphore\"\n",
    "    assert capital_case(\"python\") == \"Python\"\n",
    "    assert capital_case(\"curso\") == \"Curso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..F                                                                      [100%]\n",
      "=================================== FAILURES ===================================\n",
      "______________________________ test_comparewithCC ______________________________\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    def test_comparewithCC(supply_AA_BB_CC):\n",
      "        zz = 25\n",
      ">       assert supply_AA_BB_CC[2] == zz, \"cc and zz comparison failed\"\n",
      "E       AssertionError: cc and zz comparison failed\n",
      "E       assert 45 == 25\n",
      "\n",
      "<ipython-input-21-bc2845625045>:24: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED testing.py::test_comparewithCC - AssertionError: cc and zz comparison ...\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def supply_AA_BB_CC():\n",
    "    aa = 25\n",
    "    bb = 35\n",
    "    cc = 45\n",
    "    return [aa, bb, cc]\n",
    "\n",
    "\n",
    "def test_comparewithAA(supply_AA_BB_CC):\n",
    "    zz = 25\n",
    "    assert supply_AA_BB_CC[0] == zz, \"aa and zz comparison failed\"\n",
    "\n",
    "\n",
    "def test_comparewithBB(supply_AA_BB_CC):\n",
    "    zz = 35\n",
    "    assert supply_AA_BB_CC[1] == zz, \"bb and zz comparison failed\"\n",
    "\n",
    "\n",
    "def test_comparewithCC(supply_AA_BB_CC):\n",
    "    zz = 25\n",
    "    assert supply_AA_BB_CC[2] == zz, \"cc and zz comparison failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Crear test para las dos siguientes funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximo(valores):\n",
    "    \"\"\"Calcula el valor maximo de un iterable.\n",
    "    \n",
    "    Si se encuentra un `str` hará aosdnad\n",
    "    \n",
    "    >>> ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    max_valor = valores[0]\n",
    "\n",
    "    for val in valores:\n",
    "        if val > max_valor:\n",
    "            max_valor = val\n",
    "\n",
    "    return max_valor\n",
    "\n",
    "\n",
    "def minimo(valores):\n",
    "\n",
    "    min_valor = valores[0]\n",
    "\n",
    "    for val in valores:\n",
    "        if val < min_valor:\n",
    "            min_valor = val\n",
    "\n",
    "    return min_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximo(valores):\n",
    "    \"\"\"Calcular el valor máximo de un iterable.\n",
    "    \n",
    "    >>> maximo([1,2,3,4,5,])\n",
    "    5\n",
    "    \n",
    "    >>> maximo([123123,-2,-234234,0])\n",
    "    123123\n",
    "    \"\"\"\n",
    "    \n",
    "    return max(valores)\n",
    "\n",
    "def minimo(valores):\n",
    "    \n",
    "    return min(valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    add(2, 2)\n",
      "Expecting:\n",
      "    4\n",
      "ok\n",
      "Trying:\n",
      "    add(10, 2)\n",
      "Expecting:\n",
      "    12\n",
      "ok\n",
      "Trying:\n",
      "    add(125, 2)\n",
      "Expecting:\n",
      "    127\n",
      "ok\n",
      "Trying:\n",
      "    maximo([1,2,3,4,5,])\n",
      "Expecting:\n",
      "    5\n",
      "ok\n",
      "Trying:\n",
      "    maximo([123123,-2,-234234,0])\n",
      "Expecting:\n",
      "    123123\n",
      "ok\n",
      "7 items had no tests:\n",
      "    __main__\n",
      "    __main__.TestDemo\n",
      "    __main__.TestDemo.test\n",
      "    __main__.capital_case\n",
      "    __main__.minimo\n",
      "    __main__.supply_AA_BB_CC\n",
      "    __main__.test_min_p\n",
      "2 items passed all tests:\n",
      "   3 tests in __main__.add\n",
      "   2 tests in __main__.maximo\n",
      "5 tests in 9 items.\n",
      "5 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..                                                                       [100%]\n",
      "=============================== warnings summary ===============================\n",
      ".venv/lib/python3.7/site-packages/ipytest/_pytest_support.py:141\n",
      "  /Users/r/Projects/courses/python/basico/.venv/lib/python3.7/site-packages/ipytest/_pytest_support.py:141: PytestDeprecationWarning: direct construction of Module has been deprecated, please use Module.from_parent\n",
      "    return Module(path=path, parent=parent, module=self.module)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/latest/warnings.html\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_minimo():\n",
    "    valores = (2, 3, 1, 4, 6)\n",
    "\n",
    "    val = minimo(valores)\n",
    "    assert val == 1\n",
    "    \n",
    "    assert minimo([8,0,4]) == 0\n",
    "    \n",
    "\n",
    "\n",
    "def test_maximo():\n",
    "    valores = (2, 3, 1, 4, 6)\n",
    "\n",
    "    val = maximo(valores)\n",
    "    assert val == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.8.6, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /Users/r/Projects/teach/05_testing\n",
      "collected 2 items\n",
      "\n",
      "testing.py ..                                                            [100%]\n",
      "\n",
      "============================== 2 passed in 0.18s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -m cursopython\n",
    "\n",
    "\n",
    "@pytest.mark.cursopython\n",
    "def test_b1():\n",
    "\n",
    "    assert \"falcon\" == \"fal\" + \"con\"\n",
    "    \n",
    "    \n",
    "@pytest.mark.cursopython2\n",
    "def test_b12():\n",
    "\n",
    "    assert \"falcon\" == \"fal\" + \"con\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.8.6, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /Users/r/Projects/teach/05_testing\n",
      "collected 4 items\n",
      "\n",
      "testing.py ..F.                                                          [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "___________________________ test_min_p[valores2-asd] ___________________________\n",
      "\n",
      "valores = [30, 20, 10], resultado = 'asd'\n",
      "\n",
      "    @pytest.mark.parametrize(\n",
      "        \"valores,resultado\",\n",
      "        [\n",
      "            ([30, 20, 10], 10),\n",
      "            ([200000, -1, 12], -1),\n",
      "            ([30, 20, 10], \"asd\"),\n",
      "            ([30, 20, 10], 10),\n",
      "        ],\n",
      "    )\n",
      "    def test_min_p(valores, resultado):\n",
      "    \n",
      "        min_valor = valores[0]\n",
      "    \n",
      "        for val in valores:\n",
      "            if val < min_valor:\n",
      "                min_valor = val\n",
      "    \n",
      ">       assert min_valor == resultado\n",
      "E       AssertionError: assert 10 == 'asd'\n",
      "\n",
      "<ipython-input-32-e8c422e5cfa6>:18: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED testing.py::test_min_p[valores2-asd] - AssertionError: assert 10 == 'asd'\n",
      "========================= 1 failed, 3 passed in 0.54s ==========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"valores,resultado\",\n",
    "    [\n",
    "        ([30, 20, 10], 10),\n",
    "        ([200000, -1, 12], -1),\n",
    "        ([30, 20, 10], \"asd\"),\n",
    "        ([30, 20, 10], 10),\n",
    "    ],\n",
    ")\n",
    "def test_parser_xml(valores, resultado):\n",
    "\n",
    "    min_valor = valores[0]\n",
    "\n",
    "    for val in valores:\n",
    "        if val < min_valor:\n",
    "            min_valor = val\n",
    "\n",
    "    assert min_valor == resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestDemo(unittest.TestCase):\n",
    "    \"\"\"Example of how to use unittest in Jupyter.\"\"\"\n",
    "\n",
    "    def test(self):\n",
    "        self.assertEqual(\"foo\".upper(), \"FOO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.012s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"first-arg-is-ignored\"], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    \"\"\"\n",
    "    Suma dos elementos.\n",
    "    \n",
    "    \n",
    "    >>> add(2, 2)\n",
    "    4\n",
    "    \n",
    "    >>> add(10, 2)\n",
    "    12\n",
    "    \n",
    "    >>> add(125, 3)\n",
    "    128\n",
    "    \"\"\"\n",
    "    \n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    add(2, 2)\n",
      "Expecting:\n",
      "    4\n",
      "ok\n",
      "Trying:\n",
      "    add(10, 2)\n",
      "Expecting:\n",
      "    12\n",
      "ok\n",
      "Trying:\n",
      "    add(125, 3)\n",
      "Expecting:\n",
      "    128\n",
      "ok\n",
      "Trying:\n",
      "    datos_servers = {\n",
      "    \"server_1\": [1,2,3],\n",
      "    \"server_2\": [2,3,4],\n",
      "    \"server_3\": [4,5,6],\n",
      "    \"año\": 2020,\n",
      "    \"admin\": \"ricardo\",\n",
      "    }\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    datos_servers_2 = {\n",
      "    \"hola_server_1\": [1,2,3],\n",
      "    \"server_2\": [2,3,4],\n",
      "    \"server_3\": [4,5,6],\n",
      "    \"año\": 2020,\n",
      "    \"admin\": \"ricardo\",\n",
      "    }\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    filtrar_diccionario(datos_servers)\n",
      "Expecting:\n",
      "    ['server_1', 'server_2', 'server_3']\n",
      "ok\n",
      "Trying:\n",
      "    filtrar_diccionario(datos_servers_2)\n",
      "Expecting:\n",
      "    ['server_2', 'server_3']\n",
      "ok\n",
      "Trying:\n",
      "    filtrar_diccionario({})\n",
      "Expecting:\n",
      "    []\n",
      "ok\n",
      "4 items had no tests:\n",
      "    __main__\n",
      "    __main__.capital_case\n",
      "    __main__.supply_AA_BB_CC\n",
      "    __main__.test_min_p\n",
      "2 items passed all tests:\n",
      "   3 tests in __main__.add\n",
      "   5 tests in __main__.filtrar_diccionario\n",
      "8 tests in 6 items.\n",
      "8 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_servers = {\n",
    "    \"server_1\": [1,2,3],\n",
    "    \"server_2\": [2,3,4],\n",
    "    \"server_3\": [4,5,6],\n",
    "    \"año\": 2020,\n",
    "    \"admin\": \"ricardo\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ejercicio escribir doctest para esta función\n",
    "\n",
    "def filtrar_diccionario(dd):\n",
    "    \"\"\"\n",
    "    Devuelve las llaves de un diccionario que empiezan por 'server_'.\n",
    "    \n",
    "    Si la palabra 'server_' está en mitad de la key, no la captura,\n",
    "    solamente si está al principio.\n",
    "    \n",
    "    >>> datos_servers = {\n",
    "    ... \"server_1\": [1,2,3],\n",
    "    ... \"server_2\": [2,3,4],\n",
    "    ... \"server_3\": [4,5,6],\n",
    "    ... \"año\": 2020,\n",
    "    ... \"admin\": \"ricardo\",\n",
    "    ... }\n",
    "    \n",
    "    >>> datos_servers_2 = {\n",
    "    ... \"hola_server_1\": [1,2,3],\n",
    "    ... \"server_2\": [2,3,4],\n",
    "    ... \"server_3\": [4,5,6],\n",
    "    ... \"año\": 2020,\n",
    "    ... \"admin\": \"ricardo\",\n",
    "    ... }\n",
    "    \n",
    "    >>> filtrar_diccionario(datos_servers)\n",
    "    ['server_1', 'server_2', 'server_3']\n",
    "    \n",
    "    >>> filtrar_diccionario(datos_servers_2)\n",
    "    ['server_2', 'server_3']\n",
    "    \n",
    "    >>> filtrar_diccionario({})\n",
    "    []\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    \n",
    "    for llave in dd.keys():\n",
    "        if llave.startswith(\"server_\"):\n",
    "            keys.append(llave)\n",
    "            \n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['server_1', 'server_2', 'server_3']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtrar_diccionario(datos_servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    add(2, 2)\n",
      "Expecting:\n",
      "    4\n",
      "ok\n",
      "Trying:\n",
      "    add(10, 2)\n",
      "Expecting:\n",
      "    12\n",
      "ok\n",
      "Trying:\n",
      "    add(125, 3)\n",
      "Expecting:\n",
      "    128\n",
      "ok\n",
      "Trying:\n",
      "    datos_servers = {\n",
      "    \"server_1\": [1,2,3],\n",
      "    \"server_2\": [2,3,4],\n",
      "    \"server_3\": [4,5,6],\n",
      "    \"año\": 2020,\n",
      "    \"admin\": \"ricardo\",\n",
      "    }\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    datos_servers_2 = {\n",
      "    \"hola_server_1\": [1,2,3],\n",
      "    \"server_2\": [2,3,4],\n",
      "    \"server_3\": [4,5,6],\n",
      "    \"año\": 2020,\n",
      "    \"admin\": \"ricardo\",\n",
      "    }\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    filtrar_diccionario(datos_servers)\n",
      "Expecting:\n",
      "    ['server_1', 'server_2', 'server_3']\n",
      "ok\n",
      "Trying:\n",
      "    filtrar_diccionario(datos_servers_2)\n",
      "Expecting:\n",
      "    ['server_2', 'server_4']\n",
      "**********************************************************************\n",
      "File \"testing.ipynb\", line 29, in __main__.filtrar_diccionario\n",
      "Failed example:\n",
      "    filtrar_diccionario(datos_servers_2)\n",
      "Expected:\n",
      "    ['server_2', 'server_4']\n",
      "Got:\n",
      "    ['server_2', 'server_3']\n",
      "Trying:\n",
      "    filtrar_diccionario({})\n",
      "Expecting:\n",
      "    []\n",
      "ok\n",
      "4 items had no tests:\n",
      "    __main__\n",
      "    __main__.capital_case\n",
      "    __main__.supply_AA_BB_CC\n",
      "    __main__.test_min_p\n",
      "1 items passed all tests:\n",
      "   3 tests in __main__.add\n",
      "**********************************************************************\n",
      "1 items had failures:\n",
      "   1 of   5 in __main__.filtrar_diccionario\n",
      "8 tests in 6 items.\n",
      "7 passed and 1 failed.\n",
      "***Test Failed*** 1 failures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=1, attempted=8)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module doctest:\n",
      "\n",
      "NAME\n",
      "    doctest - Module doctest -- a framework for running examples in docstrings.\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.8/library/doctest\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    In simplest use, end each module M to be tested with:\n",
      "    \n",
      "    def _test():\n",
      "        import doctest\n",
      "        doctest.testmod()\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        _test()\n",
      "    \n",
      "    Then running the module as a script will cause the examples in the\n",
      "    docstrings to get executed and verified:\n",
      "    \n",
      "    python M.py\n",
      "    \n",
      "    This won't display anything unless an example fails, in which case the\n",
      "    failing example(s) and the cause(s) of the failure(s) are printed to stdout\n",
      "    (why not stderr? because stderr is a lame hack <0.2 wink>), and the final\n",
      "    line of output is \"Test failed.\".\n",
      "    \n",
      "    Run it with the -v switch instead:\n",
      "    \n",
      "    python M.py -v\n",
      "    \n",
      "    and a detailed report of all examples tried is printed to stdout, along\n",
      "    with assorted summaries at the end.\n",
      "    \n",
      "    You can force verbose mode by passing \"verbose=True\" to testmod, or prohibit\n",
      "    it by passing \"verbose=False\".  In either of those cases, sys.argv is not\n",
      "    examined by testmod.\n",
      "    \n",
      "    There are a variety of other ways to run doctests, including integration\n",
      "    with the unittest framework, and support for running non-Python text\n",
      "    files containing doctests.  There are also many ways to override parts\n",
      "    of doctest's default behaviors.  See the Library Reference Manual for\n",
      "    details.\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        DocTestFailure\n",
      "        UnexpectedException\n",
      "    builtins.object\n",
      "        DocTest\n",
      "        DocTestFinder\n",
      "        DocTestParser\n",
      "        DocTestRunner\n",
      "            DebugRunner\n",
      "        Example\n",
      "        OutputChecker\n",
      "    \n",
      "    class DebugRunner(DocTestRunner)\n",
      "     |  DebugRunner(checker=None, verbose=None, optionflags=0)\n",
      "     |  \n",
      "     |  Run doc tests but raise an exception as soon as there is a failure.\n",
      "     |  \n",
      "     |  If an unexpected exception occurs, an UnexpectedException is raised.\n",
      "     |  It contains the test, the example, and the original exception:\n",
      "     |  \n",
      "     |    >>> runner = DebugRunner(verbose=False)\n",
      "     |    >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n",
      "     |    ...                                    {}, 'foo', 'foo.py', 0)\n",
      "     |    >>> try:\n",
      "     |    ...     runner.run(test)\n",
      "     |    ... except UnexpectedException as f:\n",
      "     |    ...     failure = f\n",
      "     |  \n",
      "     |    >>> failure.test is test\n",
      "     |    True\n",
      "     |  \n",
      "     |    >>> failure.example.want\n",
      "     |    '42\\n'\n",
      "     |  \n",
      "     |    >>> exc_info = failure.exc_info\n",
      "     |    >>> raise exc_info[1] # Already has the traceback\n",
      "     |    Traceback (most recent call last):\n",
      "     |    ...\n",
      "     |    KeyError\n",
      "     |  \n",
      "     |  We wrap the original exception to give the calling application\n",
      "     |  access to the test and example information.\n",
      "     |  \n",
      "     |  If the output doesn't match, then a DocTestFailure is raised:\n",
      "     |  \n",
      "     |    >>> test = DocTestParser().get_doctest('''\n",
      "     |    ...      >>> x = 1\n",
      "     |    ...      >>> x\n",
      "     |    ...      2\n",
      "     |    ...      ''', {}, 'foo', 'foo.py', 0)\n",
      "     |  \n",
      "     |    >>> try:\n",
      "     |    ...    runner.run(test)\n",
      "     |    ... except DocTestFailure as f:\n",
      "     |    ...    failure = f\n",
      "     |  \n",
      "     |  DocTestFailure objects provide access to the test:\n",
      "     |  \n",
      "     |    >>> failure.test is test\n",
      "     |    True\n",
      "     |  \n",
      "     |  As well as to the example:\n",
      "     |  \n",
      "     |    >>> failure.example.want\n",
      "     |    '2\\n'\n",
      "     |  \n",
      "     |  and the actual output:\n",
      "     |  \n",
      "     |    >>> failure.got\n",
      "     |    '1\\n'\n",
      "     |  \n",
      "     |  If a failure or error occurs, the globals are left intact:\n",
      "     |  \n",
      "     |    >>> del test.globs['__builtins__']\n",
      "     |    >>> test.globs\n",
      "     |    {'x': 1}\n",
      "     |  \n",
      "     |    >>> test = DocTestParser().get_doctest('''\n",
      "     |    ...      >>> x = 2\n",
      "     |    ...      >>> raise KeyError\n",
      "     |    ...      ''', {}, 'foo', 'foo.py', 0)\n",
      "     |  \n",
      "     |    >>> runner.run(test)\n",
      "     |    Traceback (most recent call last):\n",
      "     |    ...\n",
      "     |    doctest.UnexpectedException: <DocTest foo from foo.py:0 (2 examples)>\n",
      "     |  \n",
      "     |    >>> del test.globs['__builtins__']\n",
      "     |    >>> test.globs\n",
      "     |    {'x': 2}\n",
      "     |  \n",
      "     |  But the globals are cleared if there is no error:\n",
      "     |  \n",
      "     |    >>> test = DocTestParser().get_doctest('''\n",
      "     |    ...      >>> x = 2\n",
      "     |    ...      ''', {}, 'foo', 'foo.py', 0)\n",
      "     |  \n",
      "     |    >>> runner.run(test)\n",
      "     |    TestResults(failed=0, attempted=1)\n",
      "     |  \n",
      "     |    >>> test.globs\n",
      "     |    {}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DebugRunner\n",
      "     |      DocTestRunner\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  report_failure(self, out, test, example, got)\n",
      "     |      Report that the given example failed.\n",
      "     |  \n",
      "     |  report_unexpected_exception(self, out, test, example, exc_info)\n",
      "     |      Report that the given example raised an unexpected exception.\n",
      "     |  \n",
      "     |  run(self, test, compileflags=None, out=None, clear_globs=True)\n",
      "     |      Run the examples in `test`, and display the results using the\n",
      "     |      writer function `out`.\n",
      "     |      \n",
      "     |      The examples are run in the namespace `test.globs`.  If\n",
      "     |      `clear_globs` is true (the default), then this namespace will\n",
      "     |      be cleared after the test runs, to help with garbage\n",
      "     |      collection.  If you would like to examine the namespace after\n",
      "     |      the test completes, then use `clear_globs=False`.\n",
      "     |      \n",
      "     |      `compileflags` gives the set of flags that should be used by\n",
      "     |      the Python compiler when running the examples.  If not\n",
      "     |      specified, then it will default to the set of future-import\n",
      "     |      flags that apply to `globs`.\n",
      "     |      \n",
      "     |      The output of each example is checked using\n",
      "     |      `DocTestRunner.check_output`, and the results are formatted by\n",
      "     |      the `DocTestRunner.report_*` methods.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DocTestRunner:\n",
      "     |  \n",
      "     |  __init__(self, checker=None, verbose=None, optionflags=0)\n",
      "     |      Create a new test runner.\n",
      "     |      \n",
      "     |      Optional keyword arg `checker` is the `OutputChecker` that\n",
      "     |      should be used to compare the expected outputs and actual\n",
      "     |      outputs of doctest examples.\n",
      "     |      \n",
      "     |      Optional keyword arg 'verbose' prints lots of stuff if true,\n",
      "     |      only failures if false; by default, it's true iff '-v' is in\n",
      "     |      sys.argv.\n",
      "     |      \n",
      "     |      Optional argument `optionflags` can be used to control how the\n",
      "     |      test runner compares expected output to actual output, and how\n",
      "     |      it displays failures.  See the documentation for `testmod` for\n",
      "     |      more information.\n",
      "     |  \n",
      "     |  merge(self, other)\n",
      "     |      #/////////////////////////////////////////////////////////////////\n",
      "     |      # Backward compatibility cruft to maintain doctest.master.\n",
      "     |      #/////////////////////////////////////////////////////////////////\n",
      "     |  \n",
      "     |  report_start(self, out, test, example)\n",
      "     |      Report that the test runner is about to process the given\n",
      "     |      example.  (Only displays a message if verbose=True)\n",
      "     |  \n",
      "     |  report_success(self, out, test, example, got)\n",
      "     |      Report that the given example ran successfully.  (Only\n",
      "     |      displays a message if verbose=True)\n",
      "     |  \n",
      "     |  summarize(self, verbose=None)\n",
      "     |      Print a summary of all the test cases that have been run by\n",
      "     |      this DocTestRunner, and return a tuple `(f, t)`, where `f` is\n",
      "     |      the total number of failed examples, and `t` is the total\n",
      "     |      number of tried examples.\n",
      "     |      \n",
      "     |      The optional `verbose` argument controls how detailed the\n",
      "     |      summary is.  If the verbosity is not specified, then the\n",
      "     |      DocTestRunner's verbosity is used.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DocTestRunner:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from DocTestRunner:\n",
      "     |  \n",
      "     |  DIVIDER = '***********************************************************...\n",
      "    \n",
      "    class DocTest(builtins.object)\n",
      "     |  DocTest(examples, globs, name, filename, lineno, docstring)\n",
      "     |  \n",
      "     |  A collection of doctest examples that should be run in a single\n",
      "     |  namespace.  Each `DocTest` defines the following attributes:\n",
      "     |  \n",
      "     |    - examples: the list of examples.\n",
      "     |  \n",
      "     |    - globs: The namespace (aka globals) that the examples should\n",
      "     |      be run in.\n",
      "     |  \n",
      "     |    - name: A name identifying the DocTest (typically, the name of\n",
      "     |      the object whose docstring this DocTest was extracted from).\n",
      "     |  \n",
      "     |    - filename: The name of the file that this DocTest was extracted\n",
      "     |      from, or `None` if the filename is unknown.\n",
      "     |  \n",
      "     |    - lineno: The line number within filename where this DocTest\n",
      "     |      begins, or `None` if the line number is unavailable.  This\n",
      "     |      line number is zero-based, with respect to the beginning of\n",
      "     |      the file.\n",
      "     |  \n",
      "     |    - docstring: The string that the examples were extracted from,\n",
      "     |      or `None` if the string is unavailable.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __init__(self, examples, globs, name, filename, lineno, docstring)\n",
      "     |      Create a new DocTest containing the given examples.  The\n",
      "     |      DocTest's globals are initialized with a copy of `globs`.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DocTestFailure(builtins.Exception)\n",
      "     |  DocTestFailure(test, example, got)\n",
      "     |  \n",
      "     |  A DocTest example has failed in debugging mode.\n",
      "     |  \n",
      "     |  The exception instance has variables:\n",
      "     |  \n",
      "     |  - test: the DocTest object being run\n",
      "     |  \n",
      "     |  - example: the Example object that failed\n",
      "     |  \n",
      "     |  - got: the actual output\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DocTestFailure\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, test, example, got)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class DocTestFinder(builtins.object)\n",
      "     |  DocTestFinder(verbose=False, parser=<doctest.DocTestParser object at 0x110c71e50>, recurse=True, exclude_empty=True)\n",
      "     |  \n",
      "     |  A class used to extract the DocTests that are relevant to a given\n",
      "     |  object, from its docstring and the docstrings of its contained\n",
      "     |  objects.  Doctests can currently be extracted from the following\n",
      "     |  object types: modules, functions, classes, methods, staticmethods,\n",
      "     |  classmethods, and properties.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, verbose=False, parser=<doctest.DocTestParser object at 0x110c71e50>, recurse=True, exclude_empty=True)\n",
      "     |      Create a new doctest finder.\n",
      "     |      \n",
      "     |      The optional argument `parser` specifies a class or\n",
      "     |      function that should be used to create new DocTest objects (or\n",
      "     |      objects that implement the same interface as DocTest).  The\n",
      "     |      signature for this factory function should match the signature\n",
      "     |      of the DocTest constructor.\n",
      "     |      \n",
      "     |      If the optional argument `recurse` is false, then `find` will\n",
      "     |      only examine the given object, and not any contained objects.\n",
      "     |      \n",
      "     |      If the optional argument `exclude_empty` is false, then `find`\n",
      "     |      will include tests for objects with empty docstrings.\n",
      "     |  \n",
      "     |  find(self, obj, name=None, module=None, globs=None, extraglobs=None)\n",
      "     |      Return a list of the DocTests that are defined by the given\n",
      "     |      object's docstring, or by any of its contained objects'\n",
      "     |      docstrings.\n",
      "     |      \n",
      "     |      The optional parameter `module` is the module that contains\n",
      "     |      the given object.  If the module is not specified or is None, then\n",
      "     |      the test finder will attempt to automatically determine the\n",
      "     |      correct module.  The object's module is used:\n",
      "     |      \n",
      "     |          - As a default namespace, if `globs` is not specified.\n",
      "     |          - To prevent the DocTestFinder from extracting DocTests\n",
      "     |            from objects that are imported from other modules.\n",
      "     |          - To find the name of the file containing the object.\n",
      "     |          - To help find the line number of the object within its\n",
      "     |            file.\n",
      "     |      \n",
      "     |      Contained objects whose module does not match `module` are ignored.\n",
      "     |      \n",
      "     |      If `module` is False, no attempt to find the module will be made.\n",
      "     |      This is obscure, of use mostly in tests:  if `module` is False, or\n",
      "     |      is None but cannot be found automatically, then all objects are\n",
      "     |      considered to belong to the (non-existent) module, so all contained\n",
      "     |      objects will (recursively) be searched for doctests.\n",
      "     |      \n",
      "     |      The globals for each DocTest is formed by combining `globs`\n",
      "     |      and `extraglobs` (bindings in `extraglobs` override bindings\n",
      "     |      in `globs`).  A new copy of the globals dictionary is created\n",
      "     |      for each DocTest.  If `globs` is not specified, then it\n",
      "     |      defaults to the module's `__dict__`, if specified, or {}\n",
      "     |      otherwise.  If `extraglobs` is not specified, then it defaults\n",
      "     |      to {}.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DocTestParser(builtins.object)\n",
      "     |  A class used to parse strings containing doctest examples.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_doctest(self, string, globs, name, filename, lineno)\n",
      "     |      Extract all doctest examples from the given string, and\n",
      "     |      collect them into a `DocTest` object.\n",
      "     |      \n",
      "     |      `globs`, `name`, `filename`, and `lineno` are attributes for\n",
      "     |      the new `DocTest` object.  See the documentation for `DocTest`\n",
      "     |      for more information.\n",
      "     |  \n",
      "     |  get_examples(self, string, name='<string>')\n",
      "     |      Extract all doctest examples from the given string, and return\n",
      "     |      them as a list of `Example` objects.  Line numbers are\n",
      "     |      0-based, because it's most common in doctests that nothing\n",
      "     |      interesting appears on the same line as opening triple-quote,\n",
      "     |      and so the first interesting line is called \"line 1\" then.\n",
      "     |      \n",
      "     |      The optional argument `name` is a name identifying this\n",
      "     |      string, and is only used for error messages.\n",
      "     |  \n",
      "     |  parse(self, string, name='<string>')\n",
      "     |      Divide the given string into examples and intervening text,\n",
      "     |      and return them as a list of alternating Examples and strings.\n",
      "     |      Line numbers for the Examples are 0-based.  The optional\n",
      "     |      argument `name` is a name identifying this string, and is only\n",
      "     |      used for error messages.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DocTestRunner(builtins.object)\n",
      "     |  DocTestRunner(checker=None, verbose=None, optionflags=0)\n",
      "     |  \n",
      "     |  A class used to run DocTest test cases, and accumulate statistics.\n",
      "     |  The `run` method is used to process a single DocTest case.  It\n",
      "     |  returns a tuple `(f, t)`, where `t` is the number of test cases\n",
      "     |  tried, and `f` is the number of test cases that failed.\n",
      "     |  \n",
      "     |      >>> tests = DocTestFinder().find(_TestClass)\n",
      "     |      >>> runner = DocTestRunner(verbose=False)\n",
      "     |      >>> tests.sort(key = lambda test: test.name)\n",
      "     |      >>> for test in tests:\n",
      "     |      ...     print(test.name, '->', runner.run(test))\n",
      "     |      _TestClass -> TestResults(failed=0, attempted=2)\n",
      "     |      _TestClass.__init__ -> TestResults(failed=0, attempted=2)\n",
      "     |      _TestClass.get -> TestResults(failed=0, attempted=2)\n",
      "     |      _TestClass.square -> TestResults(failed=0, attempted=1)\n",
      "     |  \n",
      "     |  The `summarize` method prints a summary of all the test cases that\n",
      "     |  have been run by the runner, and returns an aggregated `(f, t)`\n",
      "     |  tuple:\n",
      "     |  \n",
      "     |      >>> runner.summarize(verbose=1)\n",
      "     |      4 items passed all tests:\n",
      "     |         2 tests in _TestClass\n",
      "     |         2 tests in _TestClass.__init__\n",
      "     |         2 tests in _TestClass.get\n",
      "     |         1 tests in _TestClass.square\n",
      "     |      7 tests in 4 items.\n",
      "     |      7 passed and 0 failed.\n",
      "     |      Test passed.\n",
      "     |      TestResults(failed=0, attempted=7)\n",
      "     |  \n",
      "     |  The aggregated number of tried examples and failed examples is\n",
      "     |  also available via the `tries` and `failures` attributes:\n",
      "     |  \n",
      "     |      >>> runner.tries\n",
      "     |      7\n",
      "     |      >>> runner.failures\n",
      "     |      0\n",
      "     |  \n",
      "     |  The comparison between expected outputs and actual outputs is done\n",
      "     |  by an `OutputChecker`.  This comparison may be customized with a\n",
      "     |  number of option flags; see the documentation for `testmod` for\n",
      "     |  more information.  If the option flags are insufficient, then the\n",
      "     |  comparison may also be customized by passing a subclass of\n",
      "     |  `OutputChecker` to the constructor.\n",
      "     |  \n",
      "     |  The test runner's display output can be controlled in two ways.\n",
      "     |  First, an output function (`out) can be passed to\n",
      "     |  `TestRunner.run`; this function will be called with strings that\n",
      "     |  should be displayed.  It defaults to `sys.stdout.write`.  If\n",
      "     |  capturing the output is not sufficient, then the display output\n",
      "     |  can be also customized by subclassing DocTestRunner, and\n",
      "     |  overriding the methods `report_start`, `report_success`,\n",
      "     |  `report_unexpected_exception`, and `report_failure`.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, checker=None, verbose=None, optionflags=0)\n",
      "     |      Create a new test runner.\n",
      "     |      \n",
      "     |      Optional keyword arg `checker` is the `OutputChecker` that\n",
      "     |      should be used to compare the expected outputs and actual\n",
      "     |      outputs of doctest examples.\n",
      "     |      \n",
      "     |      Optional keyword arg 'verbose' prints lots of stuff if true,\n",
      "     |      only failures if false; by default, it's true iff '-v' is in\n",
      "     |      sys.argv.\n",
      "     |      \n",
      "     |      Optional argument `optionflags` can be used to control how the\n",
      "     |      test runner compares expected output to actual output, and how\n",
      "     |      it displays failures.  See the documentation for `testmod` for\n",
      "     |      more information.\n",
      "     |  \n",
      "     |  merge(self, other)\n",
      "     |      #/////////////////////////////////////////////////////////////////\n",
      "     |      # Backward compatibility cruft to maintain doctest.master.\n",
      "     |      #/////////////////////////////////////////////////////////////////\n",
      "     |  \n",
      "     |  report_failure(self, out, test, example, got)\n",
      "     |      Report that the given example failed.\n",
      "     |  \n",
      "     |  report_start(self, out, test, example)\n",
      "     |      Report that the test runner is about to process the given\n",
      "     |      example.  (Only displays a message if verbose=True)\n",
      "     |  \n",
      "     |  report_success(self, out, test, example, got)\n",
      "     |      Report that the given example ran successfully.  (Only\n",
      "     |      displays a message if verbose=True)\n",
      "     |  \n",
      "     |  report_unexpected_exception(self, out, test, example, exc_info)\n",
      "     |      Report that the given example raised an unexpected exception.\n",
      "     |  \n",
      "     |  run(self, test, compileflags=None, out=None, clear_globs=True)\n",
      "     |      Run the examples in `test`, and display the results using the\n",
      "     |      writer function `out`.\n",
      "     |      \n",
      "     |      The examples are run in the namespace `test.globs`.  If\n",
      "     |      `clear_globs` is true (the default), then this namespace will\n",
      "     |      be cleared after the test runs, to help with garbage\n",
      "     |      collection.  If you would like to examine the namespace after\n",
      "     |      the test completes, then use `clear_globs=False`.\n",
      "     |      \n",
      "     |      `compileflags` gives the set of flags that should be used by\n",
      "     |      the Python compiler when running the examples.  If not\n",
      "     |      specified, then it will default to the set of future-import\n",
      "     |      flags that apply to `globs`.\n",
      "     |      \n",
      "     |      The output of each example is checked using\n",
      "     |      `DocTestRunner.check_output`, and the results are formatted by\n",
      "     |      the `DocTestRunner.report_*` methods.\n",
      "     |  \n",
      "     |  summarize(self, verbose=None)\n",
      "     |      Print a summary of all the test cases that have been run by\n",
      "     |      this DocTestRunner, and return a tuple `(f, t)`, where `f` is\n",
      "     |      the total number of failed examples, and `t` is the total\n",
      "     |      number of tried examples.\n",
      "     |      \n",
      "     |      The optional `verbose` argument controls how detailed the\n",
      "     |      summary is.  If the verbosity is not specified, then the\n",
      "     |      DocTestRunner's verbosity is used.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  DIVIDER = '***********************************************************...\n",
      "    \n",
      "    class Example(builtins.object)\n",
      "     |  Example(source, want, exc_msg=None, lineno=0, indent=0, options=None)\n",
      "     |  \n",
      "     |  A single doctest example, consisting of source code and expected\n",
      "     |  output.  `Example` defines the following attributes:\n",
      "     |  \n",
      "     |    - source: A single Python statement, always ending with a newline.\n",
      "     |      The constructor adds a newline if needed.\n",
      "     |  \n",
      "     |    - want: The expected output from running the source code (either\n",
      "     |      from stdout, or a traceback in case of exception).  `want` ends\n",
      "     |      with a newline unless it's empty, in which case it's an empty\n",
      "     |      string.  The constructor adds a newline if needed.\n",
      "     |  \n",
      "     |    - exc_msg: The exception message generated by the example, if\n",
      "     |      the example is expected to generate an exception; or `None` if\n",
      "     |      it is not expected to generate an exception.  This exception\n",
      "     |      message is compared against the return value of\n",
      "     |      `traceback.format_exception_only()`.  `exc_msg` ends with a\n",
      "     |      newline unless it's `None`.  The constructor adds a newline\n",
      "     |      if needed.\n",
      "     |  \n",
      "     |    - lineno: The line number within the DocTest string containing\n",
      "     |      this Example where the Example begins.  This line number is\n",
      "     |      zero-based, with respect to the beginning of the DocTest.\n",
      "     |  \n",
      "     |    - indent: The example's indentation in the DocTest string.\n",
      "     |      I.e., the number of space characters that precede the\n",
      "     |      example's first prompt.\n",
      "     |  \n",
      "     |    - options: A dictionary mapping from option flags to True or\n",
      "     |      False, which is used to override default options for this\n",
      "     |      example.  Any option flags not contained in this dictionary\n",
      "     |      are left at their default value (as specified by the\n",
      "     |      DocTestRunner's optionflags).  By default, no options are set.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __init__(self, source, want, exc_msg=None, lineno=0, indent=0, options=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OutputChecker(builtins.object)\n",
      "     |  A class used to check the whether the actual output from a doctest\n",
      "     |  example matches the expected output.  `OutputChecker` defines two\n",
      "     |  methods: `check_output`, which compares a given pair of outputs,\n",
      "     |  and returns true if they match; and `output_difference`, which\n",
      "     |  returns a string describing the differences between two outputs.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  check_output(self, want, got, optionflags)\n",
      "     |      Return True iff the actual output from an example (`got`)\n",
      "     |      matches the expected output (`want`).  These strings are\n",
      "     |      always considered to match if they are identical; but\n",
      "     |      depending on what option flags the test runner is using,\n",
      "     |      several non-exact match types are also possible.  See the\n",
      "     |      documentation for `TestRunner` for more information about\n",
      "     |      option flags.\n",
      "     |  \n",
      "     |  output_difference(self, example, got, optionflags)\n",
      "     |      Return a string describing the differences between the\n",
      "     |      expected output for a given example (`example`) and the actual\n",
      "     |      output (`got`).  `optionflags` is the set of option flags used\n",
      "     |      to compare `want` and `got`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UnexpectedException(builtins.Exception)\n",
      "     |  UnexpectedException(test, example, exc_info)\n",
      "     |  \n",
      "     |  A DocTest example has encountered an unexpected exception\n",
      "     |  \n",
      "     |  The exception instance has variables:\n",
      "     |  \n",
      "     |  - test: the DocTest object being run\n",
      "     |  \n",
      "     |  - example: the Example object that failed\n",
      "     |  \n",
      "     |  - exc_info: the exception info\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UnexpectedException\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, test, example, exc_info)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    DocFileSuite(*paths, **kw)\n",
      "        A unittest suite for one or more doctest files.\n",
      "        \n",
      "        The path to each doctest file is given as a string; the\n",
      "        interpretation of that string depends on the keyword argument\n",
      "        \"module_relative\".\n",
      "        \n",
      "        A number of options may be provided as keyword arguments:\n",
      "        \n",
      "        module_relative\n",
      "          If \"module_relative\" is True, then the given file paths are\n",
      "          interpreted as os-independent module-relative paths.  By\n",
      "          default, these paths are relative to the calling module's\n",
      "          directory; but if the \"package\" argument is specified, then\n",
      "          they are relative to that package.  To ensure os-independence,\n",
      "          \"filename\" should use \"/\" characters to separate path\n",
      "          segments, and may not be an absolute path (i.e., it may not\n",
      "          begin with \"/\").\n",
      "        \n",
      "          If \"module_relative\" is False, then the given file paths are\n",
      "          interpreted as os-specific paths.  These paths may be absolute\n",
      "          or relative (to the current working directory).\n",
      "        \n",
      "        package\n",
      "          A Python package or the name of a Python package whose directory\n",
      "          should be used as the base directory for module relative paths.\n",
      "          If \"package\" is not specified, then the calling module's\n",
      "          directory is used as the base directory for module relative\n",
      "          filenames.  It is an error to specify \"package\" if\n",
      "          \"module_relative\" is False.\n",
      "        \n",
      "        setUp\n",
      "          A set-up function.  This is called before running the\n",
      "          tests in each file. The setUp function will be passed a DocTest\n",
      "          object.  The setUp function can access the test globals as the\n",
      "          globs attribute of the test passed.\n",
      "        \n",
      "        tearDown\n",
      "          A tear-down function.  This is called after running the\n",
      "          tests in each file.  The tearDown function will be passed a DocTest\n",
      "          object.  The tearDown function can access the test globals as the\n",
      "          globs attribute of the test passed.\n",
      "        \n",
      "        globs\n",
      "          A dictionary containing initial global variables for the tests.\n",
      "        \n",
      "        optionflags\n",
      "          A set of doctest option flags expressed as an integer.\n",
      "        \n",
      "        parser\n",
      "          A DocTestParser (or subclass) that should be used to extract\n",
      "          tests from the files.\n",
      "        \n",
      "        encoding\n",
      "          An encoding that will be used to convert the files to unicode.\n",
      "    \n",
      "    DocTestSuite(module=None, globs=None, extraglobs=None, test_finder=None, **options)\n",
      "        Convert doctest tests for a module to a unittest test suite.\n",
      "        \n",
      "        This converts each documentation string in a module that\n",
      "        contains doctest tests to a unittest test case.  If any of the\n",
      "        tests in a doc string fail, then the test case fails.  An exception\n",
      "        is raised showing the name of the file containing the test and a\n",
      "        (sometimes approximate) line number.\n",
      "        \n",
      "        The `module` argument provides the module to be tested.  The argument\n",
      "        can be either a module or a module name.\n",
      "        \n",
      "        If no argument is given, the calling module is used.\n",
      "        \n",
      "        A number of options may be provided as keyword arguments:\n",
      "        \n",
      "        setUp\n",
      "          A set-up function.  This is called before running the\n",
      "          tests in each file. The setUp function will be passed a DocTest\n",
      "          object.  The setUp function can access the test globals as the\n",
      "          globs attribute of the test passed.\n",
      "        \n",
      "        tearDown\n",
      "          A tear-down function.  This is called after running the\n",
      "          tests in each file.  The tearDown function will be passed a DocTest\n",
      "          object.  The tearDown function can access the test globals as the\n",
      "          globs attribute of the test passed.\n",
      "        \n",
      "        globs\n",
      "          A dictionary containing initial global variables for the tests.\n",
      "        \n",
      "        optionflags\n",
      "           A set of doctest option flags expressed as an integer.\n",
      "    \n",
      "    debug(module, name, pm=False)\n",
      "        Debug a single doctest docstring.\n",
      "        \n",
      "        Provide the module (or dotted name of the module) containing the\n",
      "        test to be debugged and the name (within the module) of the object\n",
      "        with the docstring with tests to be debugged.\n",
      "    \n",
      "    debug_src(src, pm=False, globs=None)\n",
      "        Debug a single doctest docstring, in argument `src`'\n",
      "    \n",
      "    register_optionflag(name)\n",
      "    \n",
      "    run_docstring_examples(f, globs, verbose=False, name='NoName', compileflags=None, optionflags=0)\n",
      "        Test examples in the given object's docstring (`f`), using `globs`\n",
      "        as globals.  Optional argument `name` is used in failure messages.\n",
      "        If the optional argument `verbose` is true, then generate output\n",
      "        even if there are no failures.\n",
      "        \n",
      "        `compileflags` gives the set of flags that should be used by the\n",
      "        Python compiler when running the examples.  If not specified, then\n",
      "        it will default to the set of future-import flags that apply to\n",
      "        `globs`.\n",
      "        \n",
      "        Optional keyword arg `optionflags` specifies options for the\n",
      "        testing and output.  See the documentation for `testmod` for more\n",
      "        information.\n",
      "    \n",
      "    script_from_examples(s)\n",
      "        Extract script from text with examples.\n",
      "        \n",
      "        Converts text with examples to a Python script.  Example input is\n",
      "        converted to regular code.  Example output and all other words\n",
      "        are converted to comments:\n",
      "        \n",
      "        >>> text = '''\n",
      "        ...       Here are examples of simple math.\n",
      "        ...\n",
      "        ...           Python has super accurate integer addition\n",
      "        ...\n",
      "        ...           >>> 2 + 2\n",
      "        ...           5\n",
      "        ...\n",
      "        ...           And very friendly error messages:\n",
      "        ...\n",
      "        ...           >>> 1/0\n",
      "        ...           To Infinity\n",
      "        ...           And\n",
      "        ...           Beyond\n",
      "        ...\n",
      "        ...           You can use logic if you want:\n",
      "        ...\n",
      "        ...           >>> if 0:\n",
      "        ...           ...    blah\n",
      "        ...           ...    blah\n",
      "        ...           ...\n",
      "        ...\n",
      "        ...           Ho hum\n",
      "        ...           '''\n",
      "        \n",
      "        >>> print(script_from_examples(text))\n",
      "        # Here are examples of simple math.\n",
      "        #\n",
      "        #     Python has super accurate integer addition\n",
      "        #\n",
      "        2 + 2\n",
      "        # Expected:\n",
      "        ## 5\n",
      "        #\n",
      "        #     And very friendly error messages:\n",
      "        #\n",
      "        1/0\n",
      "        # Expected:\n",
      "        ## To Infinity\n",
      "        ## And\n",
      "        ## Beyond\n",
      "        #\n",
      "        #     You can use logic if you want:\n",
      "        #\n",
      "        if 0:\n",
      "           blah\n",
      "           blah\n",
      "        #\n",
      "        #     Ho hum\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    set_unittest_reportflags(flags)\n",
      "        Sets the unittest option flags.\n",
      "        \n",
      "        The old flag is returned so that a runner could restore the old\n",
      "        value if it wished to:\n",
      "        \n",
      "          >>> import doctest\n",
      "          >>> old = doctest._unittest_reportflags\n",
      "          >>> doctest.set_unittest_reportflags(REPORT_NDIFF |\n",
      "          ...                          REPORT_ONLY_FIRST_FAILURE) == old\n",
      "          True\n",
      "        \n",
      "          >>> doctest._unittest_reportflags == (REPORT_NDIFF |\n",
      "          ...                                   REPORT_ONLY_FIRST_FAILURE)\n",
      "          True\n",
      "        \n",
      "        Only reporting flags can be set:\n",
      "        \n",
      "          >>> doctest.set_unittest_reportflags(ELLIPSIS)\n",
      "          Traceback (most recent call last):\n",
      "          ...\n",
      "          ValueError: ('Only reporting flags allowed', 8)\n",
      "        \n",
      "          >>> doctest.set_unittest_reportflags(old) == (REPORT_NDIFF |\n",
      "          ...                                   REPORT_ONLY_FIRST_FAILURE)\n",
      "          True\n",
      "    \n",
      "    testfile(filename, module_relative=True, name=None, package=None, globs=None, verbose=None, report=True, optionflags=0, extraglobs=None, raise_on_error=False, parser=<doctest.DocTestParser object at 0x110c71e80>, encoding=None)\n",
      "        Test examples in the given file.  Return (#failures, #tests).\n",
      "        \n",
      "        Optional keyword arg \"module_relative\" specifies how filenames\n",
      "        should be interpreted:\n",
      "        \n",
      "          - If \"module_relative\" is True (the default), then \"filename\"\n",
      "             specifies a module-relative path.  By default, this path is\n",
      "             relative to the calling module's directory; but if the\n",
      "             \"package\" argument is specified, then it is relative to that\n",
      "             package.  To ensure os-independence, \"filename\" should use\n",
      "             \"/\" characters to separate path segments, and should not\n",
      "             be an absolute path (i.e., it may not begin with \"/\").\n",
      "        \n",
      "          - If \"module_relative\" is False, then \"filename\" specifies an\n",
      "            os-specific path.  The path may be absolute or relative (to\n",
      "            the current working directory).\n",
      "        \n",
      "        Optional keyword arg \"name\" gives the name of the test; by default\n",
      "        use the file's basename.\n",
      "        \n",
      "        Optional keyword argument \"package\" is a Python package or the\n",
      "        name of a Python package whose directory should be used as the\n",
      "        base directory for a module relative filename.  If no package is\n",
      "        specified, then the calling module's directory is used as the base\n",
      "        directory for module relative filenames.  It is an error to\n",
      "        specify \"package\" if \"module_relative\" is False.\n",
      "        \n",
      "        Optional keyword arg \"globs\" gives a dict to be used as the globals\n",
      "        when executing examples; by default, use {}.  A copy of this dict\n",
      "        is actually used for each docstring, so that each docstring's\n",
      "        examples start with a clean slate.\n",
      "        \n",
      "        Optional keyword arg \"extraglobs\" gives a dictionary that should be\n",
      "        merged into the globals that are used to execute examples.  By\n",
      "        default, no extra globals are used.\n",
      "        \n",
      "        Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n",
      "        only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n",
      "        \n",
      "        Optional keyword arg \"report\" prints a summary at the end when true,\n",
      "        else prints nothing at the end.  In verbose mode, the summary is\n",
      "        detailed, else very brief (in fact, empty if all tests passed).\n",
      "        \n",
      "        Optional keyword arg \"optionflags\" or's together module constants,\n",
      "        and defaults to 0.  Possible values (see the docs for details):\n",
      "        \n",
      "            DONT_ACCEPT_TRUE_FOR_1\n",
      "            DONT_ACCEPT_BLANKLINE\n",
      "            NORMALIZE_WHITESPACE\n",
      "            ELLIPSIS\n",
      "            SKIP\n",
      "            IGNORE_EXCEPTION_DETAIL\n",
      "            REPORT_UDIFF\n",
      "            REPORT_CDIFF\n",
      "            REPORT_NDIFF\n",
      "            REPORT_ONLY_FIRST_FAILURE\n",
      "        \n",
      "        Optional keyword arg \"raise_on_error\" raises an exception on the\n",
      "        first unexpected exception or failure. This allows failures to be\n",
      "        post-mortem debugged.\n",
      "        \n",
      "        Optional keyword arg \"parser\" specifies a DocTestParser (or\n",
      "        subclass) that should be used to extract tests from the files.\n",
      "        \n",
      "        Optional keyword arg \"encoding\" specifies an encoding that should\n",
      "        be used to convert the file to unicode.\n",
      "        \n",
      "        Advanced tomfoolery:  testmod runs methods of a local instance of\n",
      "        class doctest.Tester, then merges the results into (or creates)\n",
      "        global Tester instance doctest.master.  Methods of doctest.master\n",
      "        can be called directly too, if you want to do something unusual.\n",
      "        Passing report=0 to testmod is especially useful then, to delay\n",
      "        displaying a summary.  Invoke doctest.master.summarize(verbose)\n",
      "        when you're done fiddling.\n",
      "    \n",
      "    testmod(m=None, name=None, globs=None, verbose=None, report=True, optionflags=0, extraglobs=None, raise_on_error=False, exclude_empty=False)\n",
      "        m=None, name=None, globs=None, verbose=None, report=True,\n",
      "           optionflags=0, extraglobs=None, raise_on_error=False,\n",
      "           exclude_empty=False\n",
      "        \n",
      "        Test examples in docstrings in functions and classes reachable\n",
      "        from module m (or the current module if m is not supplied), starting\n",
      "        with m.__doc__.\n",
      "        \n",
      "        Also test examples reachable from dict m.__test__ if it exists and is\n",
      "        not None.  m.__test__ maps names to functions, classes and strings;\n",
      "        function and class docstrings are tested even if the name is private;\n",
      "        strings are tested directly, as if they were docstrings.\n",
      "        \n",
      "        Return (#failures, #tests).\n",
      "        \n",
      "        See help(doctest) for an overview.\n",
      "        \n",
      "        Optional keyword arg \"name\" gives the name of the module; by default\n",
      "        use m.__name__.\n",
      "        \n",
      "        Optional keyword arg \"globs\" gives a dict to be used as the globals\n",
      "        when executing examples; by default, use m.__dict__.  A copy of this\n",
      "        dict is actually used for each docstring, so that each docstring's\n",
      "        examples start with a clean slate.\n",
      "        \n",
      "        Optional keyword arg \"extraglobs\" gives a dictionary that should be\n",
      "        merged into the globals that are used to execute examples.  By\n",
      "        default, no extra globals are used.  This is new in 2.4.\n",
      "        \n",
      "        Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n",
      "        only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n",
      "        \n",
      "        Optional keyword arg \"report\" prints a summary at the end when true,\n",
      "        else prints nothing at the end.  In verbose mode, the summary is\n",
      "        detailed, else very brief (in fact, empty if all tests passed).\n",
      "        \n",
      "        Optional keyword arg \"optionflags\" or's together module constants,\n",
      "        and defaults to 0.  This is new in 2.3.  Possible values (see the\n",
      "        docs for details):\n",
      "        \n",
      "            DONT_ACCEPT_TRUE_FOR_1\n",
      "            DONT_ACCEPT_BLANKLINE\n",
      "            NORMALIZE_WHITESPACE\n",
      "            ELLIPSIS\n",
      "            SKIP\n",
      "            IGNORE_EXCEPTION_DETAIL\n",
      "            REPORT_UDIFF\n",
      "            REPORT_CDIFF\n",
      "            REPORT_NDIFF\n",
      "            REPORT_ONLY_FIRST_FAILURE\n",
      "        \n",
      "        Optional keyword arg \"raise_on_error\" raises an exception on the\n",
      "        first unexpected exception or failure. This allows failures to be\n",
      "        post-mortem debugged.\n",
      "        \n",
      "        Advanced tomfoolery:  testmod runs methods of a local instance of\n",
      "        class doctest.Tester, then merges the results into (or creates)\n",
      "        global Tester instance doctest.master.  Methods of doctest.master\n",
      "        can be called directly too, if you want to do something unusual.\n",
      "        Passing report=0 to testmod is especially useful then, to delay\n",
      "        displaying a summary.  Invoke doctest.master.summarize(verbose)\n",
      "        when you're done fiddling.\n",
      "    \n",
      "    testsource(module, name)\n",
      "        Extract the test sources from a doctest docstring as a script.\n",
      "        \n",
      "        Provide the module (or dotted name of the module) containing the\n",
      "        test to be debugged and the name (within the module) of the object\n",
      "        with the doc string with tests to be debugged.\n",
      "\n",
      "DATA\n",
      "    COMPARISON_FLAGS = 63\n",
      "    DONT_ACCEPT_BLANKLINE = 2\n",
      "    DONT_ACCEPT_TRUE_FOR_1 = 1\n",
      "    ELLIPSIS = 8\n",
      "    FAIL_FAST = 1024\n",
      "    IGNORE_EXCEPTION_DETAIL = 32\n",
      "    NORMALIZE_WHITESPACE = 4\n",
      "    REPORTING_FLAGS = 1984\n",
      "    REPORT_CDIFF = 128\n",
      "    REPORT_NDIFF = 256\n",
      "    REPORT_ONLY_FIRST_FAILURE = 512\n",
      "    REPORT_UDIFF = 64\n",
      "    SKIP = 16\n",
      "    __all__ = ['register_optionflag', 'DONT_ACCEPT_TRUE_FOR_1', 'DONT_ACCE...\n",
      "    __docformat__ = 'reStructuredText en'\n",
      "    __test__ = {'_TestClass': <class 'doctest._TestClass'>, 'blank lines':...\n",
      "\n",
      "FILE\n",
      "    /usr/local/Cellar/python@3.8/3.8.3_1/Frameworks/Python.framework/Versions/3.8/lib/python3.8/doctest.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(doctest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    datos_servers = {\n",
      "    \"server_1\": [1,2,3],\n",
      "    \"server_2\": [2,3,4],\n",
      "    \"server_3\": [4,5,6],\n",
      "    \"año\": 2020,\n",
      "    \"admin\": \"ricardo\",\n",
      "    }\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    datos_servers_2 = {\n",
      "    \"hola_server_1\": [1,2,3],\n",
      "    \"server_2\": [2,3,4],\n",
      "    \"server_3\": [4,5,6],\n",
      "    \"año\": 2020,\n",
      "    \"admin\": \"ricardo\",\n",
      "    }\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    filtrar_diccionario(datos_servers)\n",
      "Expecting:\n",
      "    ['server_1', 'server_2', 'server_3']\n",
      "ok\n",
      "Trying:\n",
      "    filtrar_diccionario(datos_servers_2)\n",
      "Expecting:\n",
      "    ['server_2', 'server_3']\n",
      "ok\n",
      "Trying:\n",
      "    filtrar_diccionario({})\n",
      "Expecting:\n",
      "    []\n",
      "ok\n",
      "1 items had no tests:\n",
      "    __main__\n",
      "1 items passed all tests:\n",
      "   5 tests in __main__.filtrar_diccionario\n",
      "5 tests in 2 items.\n",
      "5 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Escribir doctests para las funciontes anteriores!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Directory structure that makes running tests easy\n",
    "Fuente: https://medium.com/@bfortuner/python-unit-testing-with-pytest-and-mock-197499c4623c\n",
    "\n",
    "/rootdir\n",
    "  /src\n",
    "    /jobitems \n",
    "      api.py \n",
    "      constants.py\n",
    "      manager.py \n",
    "      models.py \n",
    "      tasks.py\n",
    "  /tests \n",
    "    /integ_tests \n",
    "      /jobitems \n",
    "        test_manager.py\n",
    "    /unit_tests \n",
    "      /jobitems \n",
    "        test_manager.py \n",
    "requirements.py \n",
    "application.py\n",
    "\n",
    "\n",
    "How do I run these tests?\n",
    "\n",
    "\n",
    "python -m pytest tests/ (all tests)\n",
    "python -m pytest -k filenamekeyword (tests matching keyword)\n",
    "python -m pytest tests/utils/test_sample.py (single test file)\n",
    "python -m pytest tests/utils/test_sample.py::test_answer_correct (single test method)\n",
    "python -m pytest --resultlog=testlog.log tests/ (log output to file)\n",
    "python -m pytest -s tests/ (print output to console)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Monekypatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of test_module.py with source code and the test\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def getssh():\n",
    "    \"\"\"Simple function to return expanded homedir ssh path.\"\"\"\n",
    "    return Path.home() / \".ssh\"\n",
    "\n",
    "\n",
    "def test_getssh(monkeypatch):\n",
    "    # mocked return function to replace Path.home\n",
    "    # always return '/abc'\n",
    "    def mockreturn():\n",
    "        return Path(\"/abc\")\n",
    "\n",
    "    # Application of the monkeypatch to replace Path.home\n",
    "    # with the behavior of mockreturn defined above.\n",
    "    monkeypatch.setattr(Path, \"home\", mockreturn)\n",
    "\n",
    "    # Calling getssh() will use mockreturn in place of Path.home\n",
    "    # for this test with the monkeypatch.\n",
    "    x = getssh()\n",
    "    assert x == Path(\"/abc/.ssh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C:\n",
    "    def hola(self):\n",
    "        print(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n"
     ]
    }
   ],
   "source": [
    "c.hola()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(C, \"adios\", lambda x: print(\"adios\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adios\n"
     ]
    }
   ],
   "source": [
    "f.adios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teach",
   "language": "python",
   "name": "teach"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
