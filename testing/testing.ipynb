{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://files.realpython.com/media/YXhT6fA.d277d5317026.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "import ipytest\n",
    "ipytest.config(rewrite_asserts=True, magics=True)\n",
    "\n",
    "__file__ = \"testing.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [1, 2, 3] == [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "La variable num debe ser un entero pero es: 1.25\nassert False\n +  where False = isinstance(1.25, int)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Projects/courses/python/avanzado/entregable/testing.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"La variable num debe ser un entero pero es: {num}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: La variable num debe ser un entero pero es: 1.25\nassert False\n +  where False = isinstance(1.25, int)"
     ]
    }
   ],
   "source": [
    "num = 1.25\n",
    "assert isinstance(num, int), f\"La variable num debe ser un entero pero es: {num}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example():\n",
    "    assert [1, 2, 3] == [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_case(x):\n",
    "    \n",
    "    return x.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Un texto'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_case(\"un texto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.7.7, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/r/Projects/courses/python/avanzado/entregable\n",
      "collected 1 item\n",
      "\n",
      "testing.py .                                                             [100%]\n",
      "\n",
      "============================== 1 passed in 0.04s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_capital_case():\n",
    "    assert capital_case(\"semaphore\") == \"Semaphore\"\n",
    "    assert capital_case(\"python\") == \"Python\"\n",
    "    assert capital_case(\"curso\") == \"Curso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..F                                                                      [100%]\n",
      "=================================== FAILURES ===================================\n",
      "______________________________ test_comparewithCC ______________________________\n",
      "\n",
      "supply_AA_BB_CC = [25, 35, 45]\n",
      "\n",
      "    def test_comparewithCC(supply_AA_BB_CC):\n",
      "        zz = 25\n",
      ">       assert supply_AA_BB_CC[2] == zz, \"cc and zz comparison failed\"\n",
      "E       AssertionError: cc and zz comparison failed\n",
      "E       assert 45 == 25\n",
      "\n",
      "<ipython-input-15-bc2845625045>:24: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED testing.py::test_comparewithCC - AssertionError: cc and zz comparison ...\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def supply_AA_BB_CC():\n",
    "    aa = 25\n",
    "    bb = 35\n",
    "    cc = 45\n",
    "    return [aa, bb, cc]\n",
    "\n",
    "\n",
    "def test_comparewithAA(supply_AA_BB_CC):\n",
    "    zz = 25\n",
    "    assert supply_AA_BB_CC[0] == zz, \"aa and zz comparison failed\"\n",
    "\n",
    "\n",
    "def test_comparewithBB(supply_AA_BB_CC):\n",
    "    zz = 35\n",
    "    assert supply_AA_BB_CC[1] == zz, \"bb and zz comparison failed\"\n",
    "\n",
    "\n",
    "def test_comparewithCC(supply_AA_BB_CC):\n",
    "    zz = 25\n",
    "    assert supply_AA_BB_CC[2] == zz, \"cc and zz comparison failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Crear test para las dos siguientes funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximo(valores):\n",
    "    \"\"\"Calcula el valor maximo de un iterable.\n",
    "    \n",
    "    Si se encuentra un `str` hará aosdnad\n",
    "    \n",
    "    >>> ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    max_valor = valores[0]\n",
    "\n",
    "    for val in valores:\n",
    "        if val > max_valor:\n",
    "            max_valor = val\n",
    "\n",
    "    return max_valor\n",
    "\n",
    "\n",
    "def minimo(valores):\n",
    "\n",
    "    min_valor = valores[0]\n",
    "\n",
    "    for val in valores:\n",
    "        if val < min_valor:\n",
    "            min_valor = val\n",
    "\n",
    "    return min_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximo(valores):\n",
    "    \"\"\"Calcular el valor máximo de un iterable.\n",
    "    \n",
    "    >>> maximo([1,2,3,4,5,])\n",
    "    5\n",
    "    \n",
    "    >>> maximo([123123,-2,-234234,0])\n",
    "    123123\n",
    "    \"\"\"\n",
    "    \n",
    "    return max(valores)\n",
    "\n",
    "def minimo(valores):\n",
    "    \n",
    "    return min(valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    add(2, 2)\n",
      "Expecting:\n",
      "    4\n",
      "ok\n",
      "Trying:\n",
      "    add(10, 2)\n",
      "Expecting:\n",
      "    12\n",
      "ok\n",
      "Trying:\n",
      "    add(125, 2)\n",
      "Expecting:\n",
      "    127\n",
      "ok\n",
      "Trying:\n",
      "    maximo([1,2,3,4,5,])\n",
      "Expecting:\n",
      "    5\n",
      "ok\n",
      "Trying:\n",
      "    maximo([123123,-2,-234234,0])\n",
      "Expecting:\n",
      "    123123\n",
      "ok\n",
      "7 items had no tests:\n",
      "    __main__\n",
      "    __main__.TestDemo\n",
      "    __main__.TestDemo.test\n",
      "    __main__.capital_case\n",
      "    __main__.minimo\n",
      "    __main__.supply_AA_BB_CC\n",
      "    __main__.test_min_p\n",
      "2 items passed all tests:\n",
      "   3 tests in __main__.add\n",
      "   2 tests in __main__.maximo\n",
      "5 tests in 9 items.\n",
      "5 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..                                                                       [100%]\n",
      "=============================== warnings summary ===============================\n",
      ".venv/lib/python3.7/site-packages/ipytest/_pytest_support.py:141\n",
      "  /Users/r/Projects/courses/python/basico/.venv/lib/python3.7/site-packages/ipytest/_pytest_support.py:141: PytestDeprecationWarning: direct construction of Module has been deprecated, please use Module.from_parent\n",
      "    return Module(path=path, parent=parent, module=self.module)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/latest/warnings.html\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -qq\n",
    "\n",
    "def test_minimo():\n",
    "    valores = (2, 3, 1, 4, 6)\n",
    "\n",
    "    val = minimo(valores)\n",
    "    assert val == 1\n",
    "    \n",
    "    assert minimo([8,0,4]) == 0\n",
    "    \n",
    "\n",
    "\n",
    "def test_maximo():\n",
    "    valores = (2, 3, 1, 4, 6)\n",
    "\n",
    "    val = maximo(valores)\n",
    "    assert val == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.7.7, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/r/Projects/courses/python/avanzado/entregable\n",
      "collected 1 item\n",
      "\n",
      "testing.py .                                                             [100%]\n",
      "\n",
      "============================== 1 passed in 0.05s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -m cursopython\n",
    "\n",
    "\n",
    "@pytest.mark.cursopython\n",
    "def test_b1():\n",
    "\n",
    "    assert \"falcon\" == \"fal\" + \"con\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform darwin -- Python 3.7.7, pytest-5.4.1, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/r/Projects/courses/python/avanzado/entregable\n",
      "collected 4 items\n",
      "\n",
      "testing.py ....                                                          [100%]\n",
      "\n",
      "============================== 4 passed in 0.19s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"valores,resultado\",\n",
    "    [\n",
    "        ([30, 20, 10], 10),\n",
    "        ([200000, -1, 12], -1),\n",
    "        ([30, 20, 10], \"asd\"),\n",
    "        ([30, 20, 10], 10),\n",
    "    ],\n",
    ")\n",
    "def test_min_p(valores, resultado):\n",
    "\n",
    "    min_valor = valores[0]\n",
    "\n",
    "    for val in valores:\n",
    "        if val < min_valor:\n",
    "            min_valor = val\n",
    "\n",
    "    return min_valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestDemo(unittest.TestCase):\n",
    "    \"\"\"Example of how to use unittest in Jupyter.\"\"\"\n",
    "\n",
    "    def test(self):\n",
    "        self.assertEqual(\"foo\".upper(), \"FOO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.012s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"first-arg-is-ignored\"], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doctests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    \"\"\"\n",
    "    This is una suma:\n",
    "    >>> add(2, 2)\n",
    "    4\n",
    "    \n",
    "    >>> add(10, 2)\n",
    "    12\n",
    "    \n",
    "    >>> add(125, 2)\n",
    "    127\n",
    "    \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    add(2, 2)\n",
      "Expecting:\n",
      "    4\n",
      "ok\n",
      "Trying:\n",
      "    add(10, 2)\n",
      "Expecting:\n",
      "    12\n",
      "ok\n",
      "Trying:\n",
      "    add(125, 2)\n",
      "Expecting:\n",
      "    127\n",
      "ok\n",
      "8 items had no tests:\n",
      "    __main__\n",
      "    __main__.TestDemo\n",
      "    __main__.TestDemo.test\n",
      "    __main__.capital_case\n",
      "    __main__.maximo\n",
      "    __main__.minimo\n",
      "    __main__.supply_AA_BB_CC\n",
      "    __main__.test_min_p\n",
      "1 items passed all tests:\n",
      "   3 tests in __main__.add\n",
      "3 tests in 9 items.\n",
      "3 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Escribir doctests para las funciontes anteriores!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Directory structure that makes running tests easy\n",
    "Fuente: https://medium.com/@bfortuner/python-unit-testing-with-pytest-and-mock-197499c4623c\n",
    "\n",
    "/rootdir\n",
    "  /src\n",
    "    /jobitems \n",
    "      api.py \n",
    "      constants.py\n",
    "      manager.py \n",
    "      models.py \n",
    "      tasks.py\n",
    "  /tests \n",
    "    /integ_tests \n",
    "      /jobitems \n",
    "        test_manager.py\n",
    "    /unit_tests \n",
    "      /jobitems \n",
    "        test_manager.py \n",
    "requirements.py \n",
    "application.py\n",
    "\n",
    "\n",
    "How do I run these tests?\n",
    "\n",
    "\n",
    "python -m pytest tests/ (all tests)\n",
    "python -m pytest -k filenamekeyword (tests matching keyword)\n",
    "python -m pytest tests/utils/test_sample.py (single test file)\n",
    "python -m pytest tests/utils/test_sample.py::test_answer_correct (single test method)\n",
    "python -m pytest --resultlog=testlog.log tests/ (log output to file)\n",
    "python -m pytest -s tests/ (print output to console)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Monekypatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of test_module.py with source code and the test\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def getssh():\n",
    "    \"\"\"Simple function to return expanded homedir ssh path.\"\"\"\n",
    "    return Path.home() / \".ssh\"\n",
    "\n",
    "\n",
    "def test_getssh(monkeypatch):\n",
    "    # mocked return function to replace Path.home\n",
    "    # always return '/abc'\n",
    "    def mockreturn():\n",
    "        return Path(\"/abc\")\n",
    "\n",
    "    # Application of the monkeypatch to replace Path.home\n",
    "    # with the behavior of mockreturn defined above.\n",
    "    monkeypatch.setattr(Path, \"home\", mockreturn)\n",
    "\n",
    "    # Calling getssh() will use mockreturn in place of Path.home\n",
    "    # for this test with the monkeypatch.\n",
    "    x = getssh()\n",
    "    assert x == Path(\"/abc/.ssh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avanzado",
   "language": "python",
   "name": "avanzado"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
